{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Neural Network Implementation\n",
    "\n",
    "#### Current Structure of the codebase\n",
    "- Toy Dataset Generation: gen_data (checkboard, mul_circle, spiral, moon, circle, blob)\n",
    "- Activation Functions Class: AF (linear, sigmoid, tanh, relu, softmax, leaky_relu, selu)\n",
    "- Loss Function Class: LF (We will only implement Softmax + Cross Entropy this time)\n",
    "- The NN Model: SimpleNN (Defined a NN with list of neuron count and activation function)\n",
    "- Optimizer Classes: Optimizer (SGD, SGDMomentum, SGDNesterovMomentum, Adagrad, RMSprop, Adam)\n",
    "- Training Scheduler: StopScheduler (EarlyStopScheduler)\n",
    "- Learning Rate Scheduler: LRScheduler (ReduceLROnPlateau)\n",
    "- Utility Functions: grouper (Group data into mini-batches)\n",
    "- Training Loop: train_model (The main function to train a model)\n",
    "- Animation functions for the training process: draw_process, calculate_model_boundary, draw_model_boundary, draw_metrics_history\n",
    "    - You can skip understanding these\n",
    "\n",
    "#### Step 1\n",
    "- Get an understanding of the Training Loop\n",
    "    - We first call gen_data to get: class_cnt, training_data, validation_data, testing_data\n",
    "    - We define our model structure and inital our SimpleNN model\n",
    "    - We define our optimizer, lr_scheduler, stop_scheduler\n",
    "    - We run through train_model with (model, optimizer, lr_scheduler, stop_scheduler, training_data, validation_data)\n",
    "    - We can look at training process with draw_process\n",
    "    - We evaluate our model through model.evaluate and look at accuracy and model boundary using draw_model_boundary\n",
    "\n",
    "#### Step 2\n",
    "- Understand the SimpleNN model\n",
    "    - `__init__` will init the weights and biases\n",
    "    - `empty_structure` will return an empty model will all weight and biases to be zero (Will be helpful later on)\n",
    "    - `activation_func` and `loss_func` are wrappers functions to call the real functions\n",
    "    - `forward` and `backprop` are the forward and backward pass of the neural net\n",
    "        - `return_activations` in the forward pass is for usage in the backward pass\n",
    "        - We return the delta of weight and biases for this x and y in `backprop`, this will be passed to the optimzer to decide the real delta\n",
    "    - `update_one_batch_optimizer` is for getting the average delta to the model in the mini-batch\n",
    "        - We will pass the delta to the optimizer and let it decide how it will change the weight and biases\n",
    "    - `loss_one` and `loss` is for returning the loss\n",
    "    - `evaluate_one` and `evaluate` is for returning the evaluation result\n",
    "\n",
    "#### Step 3\n",
    "- Make the `Basic Run` loop work by filling in some blanks\n",
    "    - Implement forward/backward pass in SimpleNN\n",
    "        - The forward and backward pass needs to be in vector form\n",
    "    - Implement one of the Activation Functions (Eg. Relu)\n",
    "        - The parameter d means derivative\n",
    "    - Implement the Loss Function\n",
    "        - We implement Softmax + Cross Entropy combined loss\n",
    "    - Implement SGD to have one working optimizer\n",
    "        - Naive SGD will just be to multiply the delta by the learning rate and update the weight and biases\n",
    "    - Implement grouper function for making mini-batches\n",
    "\n",
    "#### Step 4\n",
    "- We shall be able to run through the `Basic Run` loop and see the training process happen!\n",
    "\n",
    "#### Step 5\n",
    "- Implment other Activation Functions\n",
    "    - linear, sigmoid, tanh, relu, softmax, leaky_relu, selu\n",
    "- Implement other Optimizers\n",
    "    - SGDMomentum, SGDNesterovMomentum, Adagrad, RMSprop, Adam\n",
    "- Implement other weights and biases initalization methods\n",
    "    - Normalized Standard Normal\n",
    "- Implment early stopping and learning rate decay\n",
    "    - EarlyStopScheduler\n",
    "    - ReduceLROnPlateau\n",
    "    \n",
    "#### Step Final\n",
    "- Finish with the experiments detailed at the bottom\n",
    "    - Try out different Toy Datasets\n",
    "        - Must: \"checkboard\", \"spiral\", \"mul_circle\"\n",
    "        - Others: \"circle\", \"moon\", \"blob\"\n",
    "    - Try out different activation functions\n",
    "        - Must: AF.linear, AF.sigmoid, AF.tanh, AF.relu\n",
    "        - Others: AF.leaky_relu, AF.selu ...\n",
    "    - Try out Different Optimizers\n",
    "        - Must: SGD, SGD+Momentum, RMSprop, Adam\n",
    "        - Others: SGD+NesterovMomentum, Adagrad\n",
    "    - Try out different Gradient Descent Methods\n",
    "        - Must: Batch, Stochastic, Mini-batch with different batch size\n",
    "    - Try out different model structures (deep vs shallow / wide vs thin)\n",
    "        - Must: (4)x1, (2)x4, (4)x2 - Others: (4)x3 ...\n",
    "    - Try out other weights and biases initalization methods\n",
    "    - Try out early stopping\n",
    "    - Try out learning rate decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libaries\n",
    "Since we imported numpy, you shall not need any more libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "from itertools import zip_longest\n",
    "from typing import List, Tuple, Callable, Optional\n",
    "\n",
    "# Vector Operations\n",
    "import numpy as np\n",
    "\n",
    "# For Generating Datasets\n",
    "from sklearn.datasets import make_circles, make_moons, make_blobs, make_gaussian_quantiles\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Progress Bar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Plotting Diagrams\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "# Plotting Animations\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase figure size\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are 6 generated datasets that you can use to try out your model\n",
    "\n",
    "- Must try:\n",
    "    - checkboard: XOR like Pattern\n",
    "    - mul_circle: class_cnt rings of Data\n",
    "    - spiral: Spiral Shape (From CS231n)\n",
    "- Others:\n",
    "    - moon: Two semi-circle\n",
    "    - circle: Two rings of Data\n",
    "    - blob: class_cnt Guassian Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_DATA_TYPE = [\"checkboard\", \"circle\", \"mul_circle\", \"moon\", \"blob\", \"spiral\"]\n",
    "\n",
    "def gen_data(name, per_class_data_cnt=500, class_cnt=3, show_data=False):\n",
    "    n_samples = per_class_data_cnt * class_cnt\n",
    "    if name == \"circle\":\n",
    "        class_cnt = 2\n",
    "        X, Y = make_circles(n_samples=n_samples, noise=0.2, factor=0.3)\n",
    "    elif name == \"mul_circle\":\n",
    "        X, Y = make_gaussian_quantiles(n_samples=n_samples, n_features=2, n_classes=class_cnt)\n",
    "    elif name == \"moon\":\n",
    "        class_cnt = 2\n",
    "        X, Y = make_moons(n_samples=n_samples, noise=0.1)\n",
    "    elif name == \"blob\":\n",
    "        X, Y = make_blobs(n_samples=n_samples, n_features=2, centers=class_cnt)\n",
    "    elif name == \"checkboard\":\n",
    "        class_cnt = 2\n",
    "        X = np.zeros((n_samples, 2))\n",
    "        Y = np.zeros(n_samples)\n",
    "        offsets = [((1, 1), 0), ((1, -1), 1), ((-1, 1), 1), ((-1, -1), 0)]\n",
    "        for bid, (offset, y) in enumerate(offsets):\n",
    "            idx = range(per_class_data_cnt*bid//2, per_class_data_cnt*(bid+1)//2)\n",
    "            X[idx] = (np.random.rand(per_class_data_cnt//2, 2) + 0.05) * np.array(offset)\n",
    "            Y[idx] = y\n",
    "    elif name == \"spiral\":\n",
    "        X = np.zeros((n_samples, 2))\n",
    "        Y = np.zeros(n_samples)\n",
    "        for cid in range(class_cnt):\n",
    "            r = np.linspace(0.0, 1, per_class_data_cnt) # radius\n",
    "            t = np.linspace(cid*4, (cid+1)*4, per_class_data_cnt) + np.random.randn(per_class_data_cnt)*0.3 # theta\n",
    "            idx = range(per_class_data_cnt*cid, per_class_data_cnt*(cid+1))\n",
    "            X[idx] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "            Y[idx] = cid\n",
    "    else:\n",
    "        raise ValueError(\"Unknown Data Name!\")\n",
    "\n",
    "    if show_data:\n",
    "        plt.scatter(X[:, 0], X[:, 1], c=Y, s=20)\n",
    "        plt.show()\n",
    "        \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "    X_train, X_valid, Y_train, Y_valid = train_test_split(X_train, Y_train, test_size=0.25)\n",
    "\n",
    "    # Change index to one hot (Assumes index 0 ~ class_cnt)\n",
    "    # [1, 2, 0, 1] -> [[0, 1, 0], [0, 0, 1], [1, 0, 0], [0, 1, 0]]\n",
    "    def one_hot(x, class_cnt):\n",
    "        oh = np.zeros((len(x), class_cnt))\n",
    "        oh[np.arange(len(x)), [int(i) for i in x]] = 1\n",
    "        return oh.reshape(-1, class_cnt, 1)\n",
    "\n",
    "    training_data = list(zip(X_train.reshape(-1, 2, 1), one_hot(Y_train, class_cnt)))\n",
    "    validation_data = list(zip(X_valid.reshape(-1, 2, 1), one_hot(Y_valid, class_cnt)))\n",
    "    testing_data = list(zip(X_test.reshape(-1, 2, 1), one_hot(Y_test, class_cnt)))\n",
    "    \n",
    "    return class_cnt, training_data, validation_data, testing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo of Spiral Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_cnt, training_data, validation_data, testing_data = gen_data(\"spiral\", show_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### List of Activation and Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation Functions\n",
    "class AF():\n",
    "    @staticmethod\n",
    "    def linear(x, d=False):\n",
    "        return 1 if d else x\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(x, d=False):\n",
    "        return sigmoid(x)*(1-sigmoid(x)) if d else 1. / (1 + np.exp(-x))\n",
    "        #pass\n",
    "\n",
    "    @staticmethod\n",
    "    def tanh(x, d=False):\n",
    "        return 1-(tanh(x))**2 if d else np.tanh(X)\n",
    "        #pass\n",
    "\n",
    "    @staticmethod\n",
    "    def relu(x, d=False):\n",
    "        return x>0 if d else np.maximum(x, 0)\n",
    "        #pass\n",
    "\n",
    "    @staticmethod\n",
    "    def leaky_relu(x, d=False):\n",
    "        ### TODO ###\n",
    "        l = 0.01\n",
    "        return np.where(x > 0, 1, l) if d else np.where(x > 0, x, x * l)\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def selu(x, d=False):\n",
    "        ### TODO ###\n",
    "        a = 1.67326\n",
    "        l = 1.0507\n",
    "        return l * np.where(x > 0, 1, a*np.exp(x)) if d else l * np.where(x > 0, x, a*np.exp(x)-a)\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(x, d=False):\n",
    "        if d:\n",
    "            print(\"We should not need this!\")\n",
    "            return -1\n",
    "        else:\n",
    "            # Need to use stable softmax\n",
    "            ### TODO ###\n",
    "            #print('before minus max\\n', x)\n",
    "            #print('max\\n', np.max(x, axis=1))\n",
    "            #x = x - np.max(x, axis=1)\n",
    "            #print('after minus max\\n', x)\n",
    "            exps = np.exp(x - np.max(x))\n",
    "            #print('exps\\n', exps)\n",
    "            #print('sum of exp\\n', np.sum(exps))\n",
    "            #return exps / np.sum(exps, axis=1).reshape(-1,1)\n",
    "            #print(exps / np.sum(exps))\n",
    "            return exps / np.sum(exps)#.reshape(-1,1)\n",
    "            #pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Functions\n",
    "class LF():\n",
    "    @staticmethod\n",
    "    def softmax_crossentropy(activation_output, y, d=False):\n",
    "        if d:\n",
    "            ### TODO ###\n",
    "            return (activation_output - y)\n",
    "            pass\n",
    "        else:\n",
    "            ### TODO ###\n",
    "            return -np.sum(y * np.log(activation_output))\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN():\n",
    "\n",
    "    def __init__(self, structure: List[int], af: Callable):\n",
    "        # Save the model structure\n",
    "        self.layer_structure = structure\n",
    "        self.num_layers = len(structure)\n",
    "        self.af = af\n",
    "\n",
    "        # Initialize the weight and bias\n",
    "        # Weight Initialization: http://cs231n.github.io/neural-networks-2/\n",
    "        # For weights:\n",
    "        #   - Standard Normal: np.random.randn\n",
    "        #   - Normalized Standard Normal: np.random.randn / np.sqrt(<previous_layer_neuron_cnt>)\n",
    "        #   - Gaussian Normal: np.random.normal\n",
    "        # For bias:\n",
    "        #   - All zeros: np.zeros\n",
    "        #   - Standard Normal: np.random.randn\n",
    "        #   - Gaussian Normal: np.random.normal\n",
    "        self.weights = [np.random.randn(nl, pl) for pl, nl in zip(structure[:-1], structure[1:])]\n",
    "        self.biases = [np.zeros((l, 1)) for l in structure[1:]]\n",
    "\n",
    "    def empty_structure(self):\n",
    "        # Return empty structure for weight and bias\n",
    "        return [np.zeros(w.shape) for w in self.weights], [np.zeros(b.shape) for b in self.biases]\n",
    "\n",
    "    def activation_func(self, l: int, *args, **kargs):\n",
    "        # Use softmax on last layer\n",
    "        return AF.softmax(*args, **kargs) if l == self.num_layers-1 else self.af(*args, **kargs)\n",
    "\n",
    "    def loss_func(self, *args, **kargs):\n",
    "        return LF.softmax_crossentropy(*args, **kargs)\n",
    "\n",
    "    def forward(self, x, return_activations=False):\n",
    "        activations, before_activations = [x], []\n",
    "        \n",
    "        # Run through each layer\n",
    "        ### TODO ###\n",
    "        #  Use self.activation_func\n",
    "        for l, (w, b) in enumerate(zip(self.weights, self.biases)):\n",
    "            W = np.dot(w, activations[-1]) + b\n",
    "            before_activations.append(W)\n",
    "            az = self.activation_func((l+1), W, d=False)\n",
    "            activations.append(az)\n",
    "            \n",
    "        # Return value history for backprop usage\n",
    "        return (activations, before_activations) if return_activations else activations[-1]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        # Init empty data structure to store delta\n",
    "        delta_w, delta_b = self.empty_structure()\n",
    "        \n",
    "        # Feed forward pass\n",
    "        activations, before_activations = self.forward(x, return_activations=True)\n",
    "        print((activations))\n",
    "\n",
    "        # Backward pass\n",
    "        ### TODO ###\n",
    "        # Use self.activation_func, self.loss_func\n",
    "        for i in range(-1, -self.num_layers, -1):\n",
    "            if i == -1:\n",
    "                delta = self.loss_func(activations[i], y, True)# * self.activation_func(0, before_activations[i], d=True)\n",
    "                #print(i, \"delta.shape\", delta.shape)\n",
    "                #print(i, \"delta value\", delta)\n",
    "                delta_w[i] = np.dot(delta, activations[i-1].T)\n",
    "            else:\n",
    "                delta = np.dot(self.weights[i+1].T, delta) * self.activation_func(0, \n",
    "                                                            before_activations[i], d=True)\n",
    "#                 print(i, \"delta.shape\", delta.shape)\n",
    "#                 print(i, \"delta.value\", delta)\n",
    "                delta_w[i] = np.dot(delta, activations[i-1].T)\n",
    "            delta_b[i] = delta\n",
    "        return delta_w, delta_b\n",
    "        \n",
    "    def update_one_batch_optimizer(self, mini_batch, optimizer):\n",
    "        # Init empty data structure to store delta\n",
    "        batch_delta_w, batch_delta_b = self.empty_structure()\n",
    "    \n",
    "        # Run through the batch of data\n",
    "        for x, ohy in mini_batch:\n",
    "            delta_w, delta_b = self.backprop(x, ohy)\n",
    "            batch_delta_w = [bd+d for bd, d in zip(batch_delta_w, delta_w)]\n",
    "            batch_delta_b = [bd+d for bd, d in zip(batch_delta_b, delta_b)]\n",
    "        \n",
    "        # Average the delta among the same batch\n",
    "        batch_delta_w = [bd/len(mini_batch) for bd in batch_delta_w]\n",
    "        batch_delta_b = [bd/len(mini_batch) for bd in batch_delta_b]\n",
    "        \n",
    "        # Change the weights and biases by using the optimizer\n",
    "        self.weights, self.biases = optimizer.step(self.weights, self.biases, batch_delta_w, batch_delta_b)\n",
    " \n",
    "    def loss_one(self, x, y):\n",
    "        return self.loss_func(self.forward(x), y)\n",
    "    \n",
    "    def loss(self, evaluate_data):\n",
    "        return sum([self.loss_one(x, y) for x, y in evaluate_data]) / len(evaluate_data)\n",
    "\n",
    "    def evaluate_one(self, x):\n",
    "        return np.argmax(self.forward(x))\n",
    "\n",
    "    def evaluate(self, evaluate_data):\n",
    "        evaluation_result = [(x, np.argmax(y), self.evaluate_one(x)) for x, y in evaluate_data]\n",
    "        accuracy = sum([y==pred for x, y, pred in evaluation_result]) / len(evaluate_data)\n",
    "        return accuracy, evaluation_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### All the Optimzers\n",
    "\n",
    "http://cs231n.github.io/neural-networks-3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer():\n",
    "    def __init__(self, empty_model_structure):\n",
    "        # Save the empty model structure\n",
    "        self.empty_model = model.empty_structure()\n",
    "\n",
    "    def step(weights, biases, batch_delta_w, batch_delta_b):\n",
    "        # Update Weight and Bias accordingly to the average of deltas of the whole batch\n",
    "        weights = [w+bd for w, bd in zip(weights, batch_delta_w)]\n",
    "        biases = [b+bd for b, bd in zip(biases, batch_delta_b)]\n",
    "        return weights, biases\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, empty_model_structure,\n",
    "                 lr: float = 0.01):\n",
    "        # Call Optimizer's init\n",
    "        super(SGD, self).__init__(empty_model_structure)\n",
    "        # Save Parameters\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self, weights, biases, batch_delta_w, batch_delta_b):\n",
    "        # Multiply delta by lr\n",
    "        ### TODO ###\n",
    "        ret_batch_delta_w = [-self.lr * w for w in batch_delta_w]\n",
    "        ret_batch_delta_b = [-self.lr * b for b in batch_delta_b]\n",
    "\n",
    "        # Update Weight and Bias accordingly\n",
    "        weights = [w+bd for w, bd in zip(weights, ret_batch_delta_w)]\n",
    "        biases = [b+bd for b, bd in zip(biases, ret_batch_delta_b)]\n",
    "        return weights, biases\n",
    "\n",
    "class SGDMomentum(Optimizer):\n",
    "    def __init__(self, empty_model_structure,\n",
    "                 lr: float = 0.01, momentum: float = 0.9):\n",
    "        # Call Optimizer's init\n",
    "        super(SGDMomentum, self).__init__(empty_model_structure)\n",
    "        # Save Parameters\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        # Initial momentum storage\n",
    "        self.momentum_w, self.momentum_b = self.empty_model\n",
    "\n",
    "    def step(self, weights, biases, batch_delta_w, batch_delta_b):\n",
    "        # Multiply delta by lr and add previous batch momentum\n",
    "        ret_batch_delta_w = [-self.lr * w + self.momentum * m_w for w, m_w in zip(batch_delta_w, self.momentum_w)]\n",
    "        ret_batch_delta_b = [-self.lr * b + self.momentum * m_b for b, m_b in zip(batch_delta_b, self.momentum_b)]\n",
    "        ### TODO ###\n",
    "\n",
    "        # Save current delta for future momentum usage\n",
    "        self.momentum_w = ret_batch_delta_w\n",
    "        self.momentum_b = ret_batch_delta_b\n",
    "        ### TODO ###\n",
    "\n",
    "        # Update Weight and Bias accordingly\n",
    "        weights = [w+bd for w, bd in zip(weights, ret_batch_delta_w)]\n",
    "        biases = [b+bd for b, bd in zip(biases, ret_batch_delta_b)]\n",
    "        return weights, biases\n",
    "\n",
    "class SGDNesterovMomentum(Optimizer):\n",
    "    def __init__(self, empty_model_structure,\n",
    "                 lr: float = 0.01, momentum: float = 0.9):\n",
    "        # Call Optimizer's init\n",
    "        super(SGDNesterovMomentum, self).__init__(empty_model_structure)\n",
    "        # Save Parameters\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        # Initial momentum storage\n",
    "        self.momentum_w, self.momentum_b = self.empty_model\n",
    "\n",
    "    def step(self, weights, biases, batch_delta_w, batch_delta_b):\n",
    "        # Save current momentum first\n",
    "        prev_momentum_w, prev_momentum_b = self.momentum_w, self.momentum_b\n",
    "\n",
    "        # Multiply delta by lr and add previous batch momentum\n",
    "        ret_batch_delta_w = [-self.lr * w + self.momentum * m_w for w, m_w in zip(batch_delta_w, prev_momentum_w)]\n",
    "        ret_batch_delta_b = [-self.lr * b + self.momentum * m_b for b, m_b in zip(batch_delta_b, prev_momentum_b)]\n",
    "        ### TODO ###\n",
    "\n",
    "        # Save current delta for future momentum usage\n",
    "        self.momentum_w = ret_batch_delta_w\n",
    "        self.momentum_b = ret_batch_delta_b\n",
    "        ### TODO ###\n",
    "\n",
    "        # Modify the delta to accomplish look ahead\n",
    "        prev_momentum_w = [w + self.momentum * m_w for w, m_w in zip(prev_momentum_w, self.momentum_w)]\n",
    "        prev_momentum_b = [b + self.momentum * m_b for b, m_b in zip(prev_momentum_b, self.momentum_b)]\n",
    "        ### TODO ###\n",
    "        \n",
    "        # Update Weight and Bias accordingly\n",
    "        weights = [w+bd for w, bd in zip(weights, ret_batch_delta_w)]\n",
    "        biases = [b+bd for b, bd in zip(biases, ret_batch_delta_b)]\n",
    "        return weights, biases\n",
    "\n",
    "class Adagrad(Optimizer):\n",
    "    EPS = 1e-6\n",
    "    def __init__(self, empty_model_structure,\n",
    "                 lr: float = 0.01):\n",
    "        # Call Optimizer's init\n",
    "        super(Adagrad, self).__init__(empty_model_structure)\n",
    "        # Save Parameters\n",
    "        self.lr = lr\n",
    "        # Initial leraning rate storage\n",
    "        self.lr_w, self.lr_b = self.empty_model\n",
    "        \n",
    "    def step(self, weights, biases, batch_delta_w , batch_delta_b):\n",
    "        # Modify learning rate for each parameter according to gradient\n",
    "        self.lr_w = [lrw + w**2 for w, lrw in zip(batch_delta_w, self.lr_w)]\n",
    "        self.lr_b = [lrb + b**2 for b, lrb in zip(batch_delta_b, self.lr_b)]\n",
    "        ### TODO ###\n",
    "\n",
    "        # Multiply by lr with respect to learning rate of each parameter\n",
    "        ret_batch_delta_w = [-self.lr * w / (np.sqrt(lrw) + self.EPS) for w, lrw in zip(batch_delta_w, self.lr_w)]\n",
    "        ret_batch_delta_b = [-self.lr * b / (np.sqrt(lrb) + self.EPS) for b, lrb in zip(batch_delta_b, self.lr_b)]\n",
    "        ### TODO ###\n",
    "            \n",
    "        # Update Weight and Bias accordingly\n",
    "        weights = [w+bd for w, bd in zip(weights, ret_batch_delta_w)]\n",
    "        biases = [b+bd for b, bd in zip(biases, ret_batch_delta_b)]\n",
    "        return weights, biases\n",
    "    \n",
    "class RMSprop(Optimizer):\n",
    "    EPS = 1e-6\n",
    "    def __init__(self, empty_model_structure,\n",
    "                 lr: float = 0.01, decay_rate: float = 0.99):\n",
    "        # Call Optimizer's init\n",
    "        super(RMSprop, self).__init__(empty_model_structure)\n",
    "        # Save Parameters\n",
    "        self.lr = lr\n",
    "        self.decay_rate = decay_rate\n",
    "        # Initial leraning rate storage\n",
    "        self.lr_w, self.lr_b = self.empty_model\n",
    "\n",
    "    def step(self, weights, biases, batch_delta_w, batch_delta_b):\n",
    "        # Modify learning rate for each parameter according to gradient\n",
    "        self.lr_w = [self.decay_rate * lrw + ((1 - self.decay_rate) * w**2) for w, lrw in zip(batch_delta_w, self.lr_w)]\n",
    "        self.lr_b = [self.decay_rate * lrb + ((1 - self.decay_rate) * b**2) for b, lrb in zip(batch_delta_b, self.lr_b)]\n",
    "        ### TODO ###\n",
    "\n",
    "        # Multiply by lr with respect to learning rate of each parameter\n",
    "        ret_batch_delta_w = [-self.lr * w / (np.sqrt(lrw) + self.EPS) for w, lrw in zip(batch_delta_w, self.lr_w)]\n",
    "        ret_batch_delta_b = [-self.lr * b / (np.sqrt(lrb) + self.EPS) for b, lrb in zip(batch_delta_b, self.lr_b)]\n",
    "        ### TODO ###\n",
    "\n",
    "        # Update Weight and Bias accordingly\n",
    "        weights = [w+bd for w, bd in zip(weights, ret_batch_delta_w)]\n",
    "        biases = [b+bd for b, bd in zip(biases, ret_batch_delta_b)]\n",
    "        return weights, biases\n",
    "    \n",
    "class Adam(Optimizer):\n",
    "    EPS = 1e-8\n",
    "    def __init__(self, empty_model_structure,\n",
    "                 lr: float = 0.01, beta1: float = 0.9, beta2: float = 0.999, bias_correction: bool = True):\n",
    "        # Call Optimizer's init\n",
    "        super(Adam, self).__init__(empty_model_structure)\n",
    "        # Save Parameters\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.bias_correction = bias_correction\n",
    "        # Initial leraning rate and momentum storage\n",
    "        self.lr_w, self.lr_b = self.empty_model\n",
    "        self.momentum_w, self.momentum_b = self.empty_model\n",
    "        if self.bias_correction:\n",
    "            # Save the amount of steps ran\n",
    "            self.optimize_steps = 1\n",
    "\n",
    "    def step(self, weights, biases, batch_delta_w, batch_delta_b):\n",
    "        # Modify momentum according to delta with decay\n",
    "        self.momentum_w = [self.beta1 * mw + (1 - self.beta1) * w / (1 - self.beta1**(self.optimize_steps)) for w, mw in zip(batch_delta_w, self.momentum_w)]\n",
    "        self.momentum_b = [self.beta1 * mb + (1 - self.beta1) * b / (1 - self.beta1**(self.optimize_steps)) for b, mb in zip(batch_delta_b, self.momentum_b)]\n",
    "        ### TODO ###\n",
    "\n",
    "        # Modify learning rate for each parameter according to gradient\n",
    "        self.lr_w = [self.beta2 * lrw + ((1 - self.beta2) * w**2) / (1 - self.beta2**(self.optimize_steps)) for w, lrw in zip(batch_delta_w, self.lr_w)]\n",
    "        self.lr_b = [self.beta2 * lrb + ((1 - self.beta2) * b**2) / (1 - self.beta2**(self.optimize_steps)) for b, lrb in zip(batch_delta_b, self.lr_b)]\n",
    "        ### TODO ###\n",
    "\n",
    "        # Multiply by lr with respect to learning rate of each parameter\n",
    "        ret_batch_delta_w = [-self.lr * mw / ((np.sqrt(lrw)) + self.EPS) for mw, lrw in zip(self.momentum_w, self.lr_w)]\n",
    "        ret_batch_delta_b = [-self.lr * mb / ((np.sqrt(lrb)) + self.EPS) for mb, lrb in zip(self.momentum_b, self.lr_b)]\n",
    "        ### TODO ###\n",
    "\n",
    "        # Update Weight and Bias accordingly\n",
    "        weights = [w+bd for w, bd in zip(weights, ret_batch_delta_w)]\n",
    "        biases = [b+bd for b, bd in zip(biases, ret_batch_delta_b)]\n",
    "        return weights, biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stopping Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopScheduler():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def step(self, validation_loss):\n",
    "        # Never Stop\n",
    "        return False\n",
    "\n",
    "class EarlyStopScheduler(StopScheduler):\n",
    "    def __init__(self, patience=10, threshold=1e-4):\n",
    "        # Save Parameters\n",
    "        self.patience = 10\n",
    "        self.threshold = threshold\n",
    "        # Save past loss\n",
    "        self.past_loss = []\n",
    "\n",
    "    def step(self, validation_loss):\n",
    "        # Append loss to past loss\n",
    "        self.past_loss.append(validation_loss)\n",
    "        \n",
    "        # Early stop if no loss improved more than threshold% in the last patience steps\n",
    "        if (len(self.past_loss) >= 2):\n",
    "            if (abs(self.past_loss[-1] - self.past_loss[-2]) <= 1e-4):\n",
    "                return True\n",
    "        ### TODO ###\n",
    "\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=77\n",
    "if (x>=10) & (x>=9): print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRScheduler():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def step(self, current_lr, validation_loss):\n",
    "        # Return the same learning rate\n",
    "        return current_lr\n",
    "\n",
    "class ReduceLROnPlateau(LRScheduler):\n",
    "    def __init__(self, decay_factor=0.5, patience=5, min_lr=0.0001, threshold=1e-4):\n",
    "        # Save Parameters\n",
    "        self.decay_factor = decay_factor\n",
    "        self.patience = patience\n",
    "        self.min_lr = min_lr\n",
    "        self.threshold = threshold\n",
    "        # Save past loss\n",
    "        self.past_loss = []\n",
    "\n",
    "    def step(self, current_lr, validation_loss):\n",
    "        # Append loss to past loss\n",
    "        self.past_loss.append(validation_loss)\n",
    "\n",
    "        # Decay learning rate if no loss improved more than threshold% in the last patience steps\n",
    "        ### TODO ###\n",
    "\n",
    "        return current_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# Group iterable into size of n for mini-batch\n",
    "# [1, 2, 3, 4, 5] -> n=2 -> [[1, 2], [3, 4], [5]]\n",
    "def grouper(iterable, n):\n",
    "    ### TODO ###\n",
    "    return itertools.zip_longest(*[iter(iterable)] * n)\n",
    "    #return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Main Training Loop\n",
    "\n",
    "This is where the whole training process comes together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model: SimpleNN,\n",
    "                optimizer: Optimizer,\n",
    "                lr_scheduler: LRScheduler,\n",
    "                earlystop_scheduler: StopScheduler,\n",
    "                training_data, validation_data,\n",
    "                epochs: int = 100,\n",
    "                mini_batch_size: int = 10,\n",
    "                log_period: int = 10,\n",
    "                draw_process: bool = False):\n",
    "\n",
    "    # Record loss and accuracy\n",
    "    loss_his, acc_his  = [], []\n",
    "    # If draw_process, store evaluation history and boundary table\n",
    "    if draw_process:\n",
    "        print(\"Draw process will take time to eval models and save boundary!\")\n",
    "        print(\"Use larger log periods to save time!\")\n",
    "        eval_his = []\n",
    "\n",
    "    # Run through the epochs\n",
    "    progress_bar = tqdm(range(epochs))\n",
    "    for epoch in progress_bar:\n",
    "        # Create batches using grouper function and run through the batches\n",
    "        for mini_batch in grouper(training_data, mini_batch_size):\n",
    "            model.update_one_batch_optimizer(mini_batch, optimizer)\n",
    "        \n",
    "        # Calculate loss and accuracy every counter epochs\n",
    "        if epoch%log_period == 0:\n",
    "            loss_his.append((model.loss(training_data), model.loss(validation_data)))\n",
    "            te, ve = model.evaluate(training_data), model.evaluate(validation_data)\n",
    "            acc_his.append((te[0], ve[0]))\n",
    "            if draw_process:\n",
    "                eval_his.append((calculate_model_boundary(model, te[1]), calculate_model_boundary(model, ve[1])))\n",
    "\n",
    "            # Update Progress Bar Description\n",
    "            #print(loss_his[-1][0])\n",
    "            #print(acc_his[-1][0])\n",
    "            desc = f\"Train Loss: {loss_his[-1][0]:.3f}, Accuracy: {acc_his[-1][0]:.3f}\"\n",
    "            #desc = f\"Accuracy: {acc_his[-1][0]:.3f}\"\n",
    "            progress_bar.set_description(desc)\n",
    "            \n",
    "            # Check if Early Stopping is needed using validation data\n",
    "            if earlystop_scheduler.step(loss_his[-1][1]):\n",
    "                print(\"Early Stopped!\")\n",
    "                return (loss_his, acc_his, eval_his) if draw_process else (loss_his, acc_his)\n",
    "                \n",
    "            # Update learning rate according to validation loss \n",
    "            optimizer.lr = lr_scheduler.step(optimizer.lr, loss_his[-1][1])\n",
    "\n",
    "    return (loss_his, acc_his, eval_his) if draw_process else (loss_his, acc_his)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These are code use to draw the animations\n",
    "\n",
    "You can skip understanding them..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_model_boundary(model: SimpleNN, evaluation_result):\n",
    "    # Separate evaluation_result into buckets\n",
    "    correct, error = [[], [], []], [[], [], []]\n",
    "    for X, Y, pred in evaluation_result:\n",
    "        bucket = correct if Y == pred else error\n",
    "        bucket[0].append(X[0].item())\n",
    "        bucket[1].append(X[1].item())\n",
    "        bucket[2].append(Y)\n",
    "    # Calculate model boundary limits\n",
    "    x_min, x_max = min(correct[0]+error[0]), max(correct[0]+error[0])\n",
    "    y_min, y_max = min(correct[1]+error[1]), max(correct[1]+error[1])\n",
    "    step = max(x_max-x_min, y_max-y_min)/100\n",
    "    xx, yy = np.meshgrid(np.arange(x_min-step*10, x_max+step*10, step), np.arange(y_min-step*10, y_max+step*10, step))\n",
    "    # Evaluate mesh grid points on model\n",
    "    z = np.array([model.evaluate_one(x.reshape(-1, 1)) for x in np.c_[xx.ravel(), yy.ravel()]]).reshape(xx.shape)\n",
    "    return (correct, error, xx, yy, z)\n",
    "\n",
    "def draw_model_boundary(model_boundary, title: str = \"Metric\", prev_ax = None):\n",
    "    correct, error, xx, yy, z = model_boundary\n",
    "    # Init Plot if no ax\n",
    "    if prev_ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    else:\n",
    "        ax = prev_ax\n",
    "    cmap=plt.cm.Spectral\n",
    "    # Plot boundary\n",
    "    ax.contourf(xx, yy, z, alpha=0.4, cmap=cmap)\n",
    "    # Plot Correct and Error Points from evaluation_result\n",
    "    ax.scatter(correct[0], correct[1], c=correct[2], s=30, marker='o', label='correct', cmap=cmap)\n",
    "    ax.scatter(error[0], error[1], c=error[2], s=30, marker='x', label='wrong', cmap=cmap)\n",
    "    # Set the limit on the chart\n",
    "    ax.set_xlim([xx.min(), xx.max()])\n",
    "    ax.set_ylim([yy.min(), yy.max()])\n",
    "    # Show legends\n",
    "    ax.legend()\n",
    "    if prev_ax is None:\n",
    "        plt.show()\n",
    "\n",
    "def draw_metrics_history(metrics: List[Tuple[float, ...]], metric_names: Tuple[str, ...],\n",
    "                         current_step: int, title: str = \"Metric\", prev_ax = None,\n",
    "                         y_lim: Optional[List[float]] = None, tolog=False, ma_step=2):\n",
    "    # Init Plot if no ax\n",
    "    if prev_ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    else:\n",
    "        ax = prev_ax\n",
    "    # Plot each of the lines\n",
    "    assert len(metrics[0]) == len(metric_names), \"Each metric shall have a name!\"\n",
    "    # Get data of each metric\n",
    "    metric_datas = np.log(np.array(metrics).T) if tolog else np.array(metrics).T\n",
    "    for metric_name, metric_data in zip(metric_names, metric_datas):\n",
    "        # Calculate moving average for better plot\n",
    "        mov_avg_metric = np.concatenate([metric_data[:ma_step-1], np.convolve(metric_data, np.ones(ma_step), 'valid') / ma_step])\n",
    "        ax.plot(mov_avg_metric[:current_step+1], label=metric_name)\n",
    "    # Set the limit on the chart\n",
    "    ax.set_xlim([0, len(metrics)-1])\n",
    "    if y_lim is not None:\n",
    "        ax.set_ylim(y_lim)\n",
    "    else:\n",
    "        ax.set_ylim([metric_datas.min()-0.5, metric_datas.max()+0.5])\n",
    "    # Set title\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    if prev_ax is None:\n",
    "        plt.show()\n",
    "\n",
    "def draw_process(model, loss_his, acc_his, eval_his, train=False, interval=200):\n",
    "    # Get the history of train or valid\n",
    "    evaluate_data_id = 0 if train else 1\n",
    "    evaluate_data = [his[evaluate_data_id] for his in eval_his]\n",
    "    print(\"This will take some time to plot!\")\n",
    "    print(f\"Total plots: {len(evaluate_data)}\")\n",
    "    # Prepare the animation\n",
    "    fig = plt.figure(constrained_layout=True)\n",
    "    gs = gridspec.GridSpec(ncols=2, nrows=5, figure=fig)\n",
    "    ax1 = fig.add_subplot(gs[:-1, :])\n",
    "    ax2 = fig.add_subplot(gs[-1, 0])\n",
    "    ax3 = fig.add_subplot(gs[-1, 1])\n",
    "    def update(i):\n",
    "        # Draw Model Boundary\n",
    "        ax1.clear()\n",
    "        ax1_title = f'Train Data Evaluation History: {i:04}' if train else f'Validation Data Evaluation History: {i:04}'\n",
    "        draw_model_boundary(evaluate_data[i], ax1_title, prev_ax=ax1)\n",
    "        # Draw Loss Change\n",
    "        ax2.clear()\n",
    "        ax2_title = \"Loss History (Y is log of loss)\"\n",
    "        draw_metrics_history(loss_his, (\"Train\", \"Valid\"), i, ax2_title, prev_ax=ax2, ma_step=2, tolog=True)\n",
    "        # Draw Accuracy Change\n",
    "        ax3.clear()\n",
    "        ax3_title = \"Accuracy History\"\n",
    "        draw_metrics_history(acc_his, (\"Train\", \"Valid\"), i, ax3_title, prev_ax=ax3, ma_step=2, y_lim=[0, 1])\n",
    "    anim = FuncAnimation(fig, update, frames=range(0, len(evaluate_data)), interval=interval, blit=False)\n",
    "    return HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# Generate the dataset\n",
    "# class_cnt, training_data, validation_data, testing_data = gen_data(\"mul_circle\", class_cnt=3)\n",
    "class_cnt, training_data, validation_data, testing_data = gen_data(\"mul_circle\", class_cnt=2)\n",
    "print(f\"Class Count: {class_cnt}\")\n",
    "\n",
    "# Setup the model structure\n",
    "#network = [2] + [4, 4] + [class_cnt]  # Input Dimension + Layers + Output Class Count\n",
    "network = [2] + [3, 3] + [class_cnt]  # Input Dimension + Layers + Output Class Count\n",
    "print(network)\n",
    "model = SimpleNN(network, AF.relu)\n",
    "\n",
    "# Setup the optimzer and schedulers\n",
    "optimizer = Adam(model.empty_structure(), lr=0.01)\n",
    "lr_scheduler = LRScheduler()\n",
    "stop_scheduler = EarlyStopScheduler(patience=10)\n",
    "\n",
    "# Run the training process\n",
    "loss_his, acc_his, eval_his = train_model(model, optimizer, lr_scheduler, stop_scheduler,\n",
    "                                          training_data, validation_data,\n",
    "                                          epochs=100, mini_batch_size=10, log_period=10, draw_process=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the training process\n",
    "draw_process(model, loss_his, acc_his, eval_his)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1,2,3]\n",
    "if x[-1] - x[-2] <= 1:\n",
    "    print('fuck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation on testing data\n",
    " accuracy, evaluation_result = model.evaluate(testing_data)\n",
    " print(\"Accuracy:\", accuracy)\n",
    " draw_model_boundary(calculate_model_boundary(model, evaluation_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments\n",
    "\n",
    "Do some experiments on the effect of different settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try out different daasets\n",
    "- Must: \"checkboard\", \"spiral\", \"mul_circle\"\n",
    "- Others: \"circle\", \"moon\", \"blob\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try out different activation functions\n",
    "- Must: AF.linear, AF.sigmoid, AF.tanh, AF.relu\n",
    "- Others: AF.leaky_relu, AF.selu ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try out different model structures (deep vs shallow / wide vs thin)\n",
    "- Must: (4)x1, (2)x4, (4)x2\n",
    "- Others: (4)x3 ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try out different Gradient Descent Methods\n",
    "- Must: Batch, Stochastic, Mini-batch with different batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try out Different Optimizers\n",
    "- Must: SGD, SGD+Momentum, RMSprop, Adam\n",
    "- Others: SGD+NesterovMomentum, Adagrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Must: Try out Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Must: Try out Learning Rate Adjustment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Others: Try out different weight and bias initialization\n",
    "- Eg. weights: Standard Normal, Normalized Standard Normal, Gaussian Normal\n",
    "- Eg. bias: All zeros, Standard Normal, Gaussian Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
